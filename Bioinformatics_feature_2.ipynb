{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efd8288-e441-40f4-a764-f257063b8302",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas biopython requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38740d86-5997-49e0-9680-64db744e4d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Store each fasta result in a separate file \"Is not used\"\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "# === Step 1: Load your CSV file ===\n",
    "csv_path = 'mutation_matrix_features.csv' \n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Ensure Mutation column exists and is formatted like 'L112P'\n",
    "def extract_position(mutation):\n",
    "    match = re.search(r'([A-Z])(\\d+)([A-Z])', mutation)\n",
    "    return int(match.group(2)) if match else None\n",
    "\n",
    "df['Position'] = df['Mutation'].apply(extract_position)\n",
    "\n",
    "# === Step 2: Fetch FASTA sequences for each unique UniProt ID ===\n",
    "def fetch_uniprot_fasta(uniprot_id):\n",
    "    url = f'https://www.uniprot.org/uniprot/{uniprot_id}.fasta'\n",
    "    response = requests.get(url)\n",
    "    if response.ok:\n",
    "        return response.text\n",
    "    else:\n",
    "        print(f\" Failed to fetch {uniprot_id}\")\n",
    "        return None\n",
    "\n",
    "fasta_dir = Path('fasta_sequences')\n",
    "fasta_dir.mkdir(exist_ok=True)\n",
    "\n",
    "unique_ids = df['UniProt ID'].unique()\n",
    "\n",
    "for uid in unique_ids:\n",
    "    fasta = fetch_uniprot_fasta(uid)\n",
    "    if fasta:\n",
    "        with open(fasta_dir / f\"{uid}.fasta\", 'w') as f:\n",
    "            f.write(fasta)\n",
    "\n",
    "print(\" FASTA files downloaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3af34b-4cf6-4c2a-8e8f-e9f0a3231734",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Store each 1000 fasta result in a separate file \"This was just a check if it works well\"\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "# === Step 1: Load your CSV file ===\n",
    "csv_path = 'mutation_matrix_features.csv' \n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Ensure Mutation column exists and is formatted like 'L112P'\n",
    "def extract_position(mutation):\n",
    "    match = re.search(r'([A-Z])(\\d+)([A-Z])', mutation)\n",
    "    return int(match.group(2)) if match else None\n",
    "\n",
    "df['Position'] = df['Mutation'].apply(extract_position)\n",
    "\n",
    "# === Step 2: Fetch FASTA sequences for each unique UniProt ID ===\n",
    "def fetch_uniprot_fasta(uniprot_id):\n",
    "    url = f'https://www.uniprot.org/uniprot/{uniprot_id}.fasta'\n",
    "    response = requests.get(url)\n",
    "    if response.ok:\n",
    "        return response.text\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "fasta_dir = Path('fasta_sequences')\n",
    "fasta_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# === Step 3: Process in chunks of 1000 rows ===\n",
    "chunk_size = 1000\n",
    "start_index = 0\n",
    "chunk_num = 1\n",
    "\n",
    "while start_index < len(df):\n",
    "    # Select the next 1000 rows (or the remaining rows if less than 1000)\n",
    "    chunk = df.iloc[start_index:start_index + chunk_size]\n",
    "    \n",
    "    # Fetch the FASTA sequences for this chunk\n",
    "    fasta_content = \"\"\n",
    "    failed_rows = 0  # Counter to track failed rows for this chunk\n",
    "    for _, row in chunk.iterrows():\n",
    "        uniprot_id = row['UniProt ID']\n",
    "        fasta = fetch_uniprot_fasta(uniprot_id)\n",
    "        if fasta:\n",
    "            fasta_content += fasta + \"\\n\"  # Add the FASTA data for this ID\n",
    "        else:\n",
    "            failed_rows += 1  # If fetching fails, increment the counter\n",
    "    \n",
    "    # If there are FASTA sequences, save to a file\n",
    "    if fasta_content:\n",
    "        with open(fasta_dir / f\"fasta_{chunk_num}.fasta\", 'w') as f:\n",
    "            f.write(fasta_content)\n",
    "        print(f\" Chunk {chunk_num}: {len(chunk) - failed_rows}/{len(chunk)} FASTA sequences saved successfully.\")\n",
    "    else:\n",
    "        print(f\" Chunk {chunk_num}: No FASTA sequences were saved (all requests failed).\")\n",
    "\n",
    "    # Move to the next chunk\n",
    "    start_index += chunk_size\n",
    "    chunk_num += 1\n",
    "\n",
    "print(\" FASTA files downloaded and saved in chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ba36be-e78f-4f04-8e1e-94b8cf639dfb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "# === Step 1: Load your CSV file ===\n",
    "csv_path = 'mutation_matrix_features.csv' \n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Ensure Mutation column exists and is formatted like 'L112P'\n",
    "def extract_position(mutation):\n",
    "    match = re.search(r'([A-Z])(\\d+)([A-Z])', mutation)\n",
    "    return int(match.group(2)) if match else None\n",
    "\n",
    "df['Position'] = df['Mutation'].apply(extract_position)\n",
    "\n",
    "# === Step 2: Fetch FASTA sequences for each unique UniProt ID ===\n",
    "def fetch_uniprot_fasta(uniprot_id):\n",
    "    url = f'https://www.uniprot.org/uniprot/{uniprot_id}.fasta'\n",
    "    response = requests.get(url)\n",
    "    if response.ok:\n",
    "        return response.text\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# === Step 3: Extract mutated region from FASTA sequence ===\n",
    "def extract_mutated_region(fasta, mutation, position, region_size=10):\n",
    "    \"\"\"\n",
    "    Extracts the mutated region of the protein sequence based on mutation position\n",
    "    and the surrounding context.\n",
    "    \"\"\"\n",
    "    # Find the actual mutation (e.g., L112P) and extract sequence\n",
    "    mutated_sequence = \"\"\n",
    "    lines = fasta.splitlines()\n",
    "    sequence = ''.join(lines[1:])  # Join sequence lines together\n",
    "\n",
    "    # Convert position to 0-based indexing\n",
    "    position = position - 1\n",
    "\n",
    "    # Extract a region around the mutation site\n",
    "    start = max(0, position - region_size)\n",
    "    end = min(len(sequence), position + region_size + 1)\n",
    "    mutated_sequence = sequence[start:end]\n",
    "\n",
    "    # Modify the sequence according to the mutation\n",
    "    mutated_sequence = mutated_sequence[:position - start] + mutation[1] + mutated_sequence[position - start + 1:]\n",
    "\n",
    "    return mutated_sequence\n",
    "\n",
    "fasta_dir = Path('fasta_sequences')\n",
    "fasta_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# === Step 4: Process in chunks of 1000 rows ===\n",
    "chunk_size = 1000\n",
    "start_index = 0\n",
    "chunk_num = 1\n",
    "\n",
    "while start_index < len(df):\n",
    "    # Select the next 1000 rows (or the remaining rows if less than 1000)\n",
    "    chunk = df.iloc[start_index:start_index + chunk_size]\n",
    "    \n",
    "    # Fetch the FASTA sequences for this chunk\n",
    "    fasta_content = \"\"\n",
    "    failed_rows = 0  # Counter to track failed rows for this chunk\n",
    "    for _, row in chunk.iterrows():\n",
    "        uniprot_id = row['UniProt ID']\n",
    "        mutation = row['Mutation']\n",
    "        position = row['Position']\n",
    "        \n",
    "        # Fetch full FASTA sequence\n",
    "        fasta = fetch_uniprot_fasta(uniprot_id)\n",
    "        \n",
    "        if fasta:\n",
    "            mutated_sequence = extract_mutated_region(fasta, mutation, position)\n",
    "            fasta_content += f\">{uniprot_id}_{mutation}\\n{mutated_sequence}\\n\"  # Add the mutated FASTA data\n",
    "        else:\n",
    "            failed_rows += 1  # If fetching fails, increment the counter\n",
    "    \n",
    "    # If there are FASTA sequences, save to a file\n",
    "    if fasta_content:\n",
    "        with open(fasta_dir / f\"fasta_mutated_{chunk_num}.fasta\", 'w') as f:\n",
    "            f.write(fasta_content)\n",
    "        print(f\" Chunk {chunk_num}: {len(chunk) - failed_rows}/{len(chunk)} FASTA mutated sequences saved successfully.\")\n",
    "    else:\n",
    "        print(f\" Chunk {chunk_num}: No FASTA sequences were saved (all requests failed).\")\n",
    "\n",
    "    # Move to the next chunk\n",
    "    start_index += chunk_size\n",
    "    chunk_num += 1\n",
    "\n",
    "print(\" Mutated FASTA files downloaded and saved in chunks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a357d7c6-c0cd-41dd-b419-e95d7cca0876",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990d50aa-8262-4eed-91ed-67284cc8dffd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec321f8-961a-4a39-9302-5e0831ffe0dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21535a09-56e0-4630-a578-267b23ed322d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18/5/2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178fc748-3a65-40a6-8f05-9df16f371292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the results from NetSurfP into one file, and then  I will merge this result including the new features \n",
    "# with the first feature and the original data\n",
    "# During the merging, the mutation \"row\" which has no results from NetSurfP, I will have its value as NAN.\n",
    "\n",
    "\n",
    "## This code is to merge the netsurfp files together, but I will find another better way, because each file contains 19000\n",
    "## rows. \n",
    "## I WON'T USE THIS, IT'S JUST A TRIAL.\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the directory containing NetSurfP CSV files\n",
    "netsurfp_dir = Path(\"netsurfp_outputs\")\n",
    "\n",
    "# List and sort CSV files by their festa number\n",
    "csv_files = sorted(\n",
    "    netsurfp_dir.glob(\"*.csv\"),\n",
    "    key=lambda x: int(x.stem.replace(\"festa\", \"\"))\n",
    ")\n",
    "\n",
    "# Prepare list to hold valid DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Process each file safely\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file)\n",
    "        print(f\" Loaded: {file.name} | Rows: {len(df)}\")\n",
    "        dataframes.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\" Skipped {file.name} due to error: {e}\")\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "netsurfp_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Final output\n",
    "print(f\"\\n Total merged NetSurfP rows: {len(netsurfp_df)}\")\n",
    "print(netsurfp_df.head())\n",
    "\n",
    "\n",
    "# Save the merged DataFrame to a CSV file\n",
    "netsurfp_df.to_csv(\"merged_netsurfp_results.csv\", index=False)\n",
    "print(\"Merged result saved to: merged_netsurfp_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de03ef58-6548-44c3-88a6-55d1246e938a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b204912d-d0bd-4bef-89ca-84e25bf57ada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fca7cc-aa62-4f5b-9ca9-30d63cc8f696",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c855982-e244-4fc8-b9fe-42436d27c310",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca5eebc-9c06-4c33-8197-adc2ae32f01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This code flattens the netsurfp result files. I tested it for one file and then I will do this to all 19 files.\n",
    "## And group them together.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load CSV\n",
    "df_netsurf = pd.read_csv(\"festa8.csv\", sep=\",\")\n",
    "df_netsurf.columns = df_netsurf.columns.str.strip()\n",
    "\n",
    "features_per_residue = [\n",
    "    \"rsa\", \"asa\", \n",
    "    \"p[q3_H]\", \"p[q3_E]\", \"p[q3_C]\",\n",
    "    \"p[q8_G]\", \"p[q8_H]\", \"p[q8_I]\", \"p[q8_B]\", \"p[q8_E]\", \"p[q8_S]\", \"p[q8_T]\", \"p[q8_C]\",\n",
    "    \"phi\", \"psi\", \"disorder\"\n",
    "]\n",
    "\n",
    "rel_positions = list(range(-10, 0)) + list(range(1, 11))  # 20 positions total\n",
    "\n",
    "def flatten_window(group):\n",
    "    group = group.sort_values(\"n\").reset_index(drop=True)\n",
    "\n",
    "    feature_dict = {}\n",
    "    for i, pos in enumerate(rel_positions):\n",
    "        if i < len(group):  # If residue exists at this position\n",
    "            for feat in features_per_residue:\n",
    "                col_name = f\"{feat}_{pos}\"\n",
    "                feature_dict[col_name] = group.iloc[i][feat]\n",
    "        else:  # Missing residue: fill with NaN\n",
    "            for feat in features_per_residue:\n",
    "                col_name = f\"{feat}_{pos}\"\n",
    "                feature_dict[col_name] = np.nan\n",
    "\n",
    "    feature_dict[\"id\"] = group.iloc[0][\"id\"]\n",
    "    return pd.Series(feature_dict)\n",
    "\n",
    "# Apply to all groups\n",
    "df_flattened = (\n",
    "    df_netsurf.groupby(\"id\")\n",
    "    .apply(flatten_window)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Save\n",
    "df_flattened.to_csv(\"festa8_flattened_full.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b169aed-4828-41f2-94e9-453a8843e931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten all netsurfp files, for each mutation.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Folder containing all netsurfp output CSV files\n",
    "folder_path = \"netsurfp_outputs\"\n",
    "\n",
    "# Features to extract per residue\n",
    "features_per_residue = [\n",
    "    \"rsa\", \"asa\", \n",
    "    \"p[q3_H]\", \"p[q3_E]\", \"p[q3_C]\",\n",
    "    \"p[q8_G]\", \"p[q8_H]\", \"p[q8_I]\", \"p[q8_B]\", \"p[q8_E]\", \"p[q8_S]\", \"p[q8_T]\", \"p[q8_C]\",\n",
    "    \"phi\", \"psi\", \"disorder\"\n",
    "]\n",
    "\n",
    "# Relative positions: -10 to -1 and 1 to 10 (excluding 0)\n",
    "rel_positions = list(range(-10, 0)) + list(range(1, 11))\n",
    "\n",
    "# Flattening function for one group\n",
    "def flatten_window(group):\n",
    "    group = group.sort_values(\"n\").reset_index(drop=True)\n",
    "    feature_dict = {}\n",
    "    for i, pos in enumerate(rel_positions):\n",
    "        if i < len(group):\n",
    "            for feat in features_per_residue:\n",
    "                col_name = f\"{feat}_{pos}\"\n",
    "                feature_dict[col_name] = group.iloc[i][feat]\n",
    "        else:\n",
    "            for feat in features_per_residue:\n",
    "                col_name = f\"{feat}_{pos}\"\n",
    "                feature_dict[col_name] = np.nan\n",
    "    feature_dict[\"id\"] = group.iloc[0][\"id\"]\n",
    "    return pd.Series(feature_dict)\n",
    "\n",
    "# Collect all results\n",
    "all_flattened = []\n",
    "\n",
    "# Iterate over all CSV files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        df.columns = df.columns.str.strip()\n",
    "        \n",
    "        # Apply flattening per mutation group\n",
    "        flattened = (\n",
    "            df.groupby(\"id\")\n",
    "            .apply(flatten_window)\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        all_flattened.append(flattened)\n",
    "\n",
    "# Combine all data\n",
    "combined_df = pd.concat(all_flattened, ignore_index=True)\n",
    "\n",
    "# Save the combined flattened output\n",
    "combined_df.to_csv(\"all_flattened_netsurf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6cb33c-80ba-4e49-97c2-855221d86aa2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Decrease the number of columns \"Surrounding neighbors of the mutated amino acid\" \n",
    "# due to lack of memory and huge size.\n",
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"all_flattened_netsurf.csv\")\n",
    "\n",
    "# Keep only columns from -5 to 5 and 'id'\n",
    "columns_to_keep = []\n",
    "\n",
    "# Check all columns except the last one (which is 'id')\n",
    "for col in df.columns:\n",
    "    if col == 'id':\n",
    "        columns_to_keep.append(col)\n",
    "    else:\n",
    "        # Extract the position suffix from the column name\n",
    "        try:\n",
    "            pos = int(col.split('_')[-1])\n",
    "            if -5 <= pos <= 5:\n",
    "                columns_to_keep.append(col)\n",
    "        except ValueError:\n",
    "            pass  # Skip columns that don't end with a position\n",
    "\n",
    "# Filter the DataFrame\n",
    "filtered_df = df[columns_to_keep]\n",
    "\n",
    "# Save to a new file\n",
    "filtered_df.to_csv(\"smaller_all_flattened_netsurf.csv\", index=False)\n",
    "\n",
    "# Print confirmation\n",
    "print(f\"Columns kept: {len(filtered_df.columns)}\")\n",
    "print(\"Columns:\")\n",
    "for i, col in enumerate(filtered_df.columns, 1):\n",
    "    print(f\"{i}. {col}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaa68d2-97f6-4c72-8c2b-72e9b9ad9389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the 2 files of the 2 features based on the uniprot id and the mutation together.\n",
    "import pandas as pd\n",
    "\n",
    "# Load both files\n",
    "netsurf_df = pd.read_csv(\"smaller_all_flattened_netsurf.csv\")\n",
    "mutation_df = pd.read_csv(\"mutation_matrix_features.csv\")\n",
    "\n",
    "# Extract UniProt ID and Mutation from netsurf 'id' column (format: >A0AVI4_R195W)\n",
    "netsurf_df[['UniProt ID', 'Mutation']] = netsurf_df['id'].str.extract(r'>([^_]+)_([A-Z]\\d+[A-Z])')\n",
    "\n",
    "# Strip whitespace just in case\n",
    "netsurf_df['UniProt ID'] = netsurf_df['UniProt ID'].str.strip()\n",
    "netsurf_df['Mutation'] = netsurf_df['Mutation'].str.strip()\n",
    "mutation_df['UniProt ID'] = mutation_df['UniProt ID'].str.strip()\n",
    "mutation_df['Mutation'] = mutation_df['Mutation'].str.strip()\n",
    "\n",
    "# Merge on both UniProt ID and Mutation\n",
    "merged_df = pd.merge(netsurf_df, mutation_df, on=[\"UniProt ID\", \"Mutation\"], how=\"inner\")\n",
    "\n",
    "# Save the merged result\n",
    "merged_df.to_csv(\"D:/faraah_my_final_merged_featuress.csv\", index=False)\n",
    "\n",
    "print(\"Merging complete. Output saved as 'faraah_my_final_merged_featuress.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
